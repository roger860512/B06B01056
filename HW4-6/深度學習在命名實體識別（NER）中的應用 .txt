1. NER 簡介
NER又稱作專名識別，是自然語言處理中的一項基礎任務，應用範圍非常廣泛。命名實體一般指的是文本中具有特定意義或者指代性強的實體，通常包括人名、地名、組織機構名、日期時間、專有名詞等。NER系統就是從非結構化的輸入文本中抽取出上述實體，並且可以按照業務需求識別出更多類別的實體，比如產品名稱、型號、價格等。因此實體這個概念可以很廣，只要是業務需要的特殊文本片段都可以稱為實體。
學術上NER所涉及的命名實體一般包括3大類（實體類，時間類，數字類）和7小類（人名、地名、組織機構名、時間、日期、貨幣、百分比）。

實際應用中，NER模型通常只要識別出人名、地名、組織機構名、日期時間即可，一些系統還會給出專有名詞結果（比如縮寫、會議名、產品名等）。貨幣、百分比等數字類實體可通過正則搞定。另外，在一些應用場景下會給出特定領域內的實體，如書名、歌曲名、期刊名等。
NER是NLP中一項基礎性關鍵任務。從自然語言處理的流程來看，NER可以看作詞法分析中未登錄詞識別的一種，是未登錄詞中數量最多、識別難度最大、對分詞效果影響最大問題。同時NER也是關係抽取、事件抽取、知識圖譜、機器翻譯、問答系統等諸多NLP任務的基礎。
NER當前並不算是一個大熱的研究課題，因為學術界部分學者認為這是一個已經解決的問題。當然也有學者認為這個問題還沒有得到很好地解決，原因主要有：命名實體識別只是在有限的文本類型（主要是新聞語料中）和實體類別（主要是人名、地名、組織機構名）中取得了不錯的效果；與其他信息檢索領域相比，實體命名評測預料較小，容易產生過擬合；命名實體識別更側重高召回率，但在信息檢索領域，高準確率更重要；通用的識別多種類型的命名實體的系統性能很差。

2. 深度學習方法在NER中的應用
NER一直是NLP領域中的研究熱點，從早期基於詞典和規則的方法，到傳統機器學習的方法，到近年來基於深度學習的方法，NER研究進展的大概趨勢大致如下圖所示。

在基於機器學習的方法中，NER被當作序列標注問題。利用大規模語料來學習出標注模型，從而對句子的各個位置進行標注。NER 任務中的常用模型包括生成式模型HMM、判別式模型CRF等。條件隨機場（ConditionalRandom Field，CRF）是NER目前的主流模型。它的目標函數不僅考慮輸入的狀態特徵函數，而且還包含了標籤轉移特徵函數。在訓練時可以使用SGD學習模型參數。在已知模型時，給輸入序列求預測輸出序列即求使目標函數最大化的最優序列，是一個動態規劃問題，可以使用Viterbi算法解碼來得到最優標籤序列。CRF的優點在於其為一個位置進行標注的過程中可以利用豐富的內部及上下文特徵信息。

近年來，隨著硬件計算能力的發展以及詞的分布式表示（word embedding）的提出，神經網絡可以有效處理許多NLP任務。這類方法對於序列標注任務（如CWS、POS、NER）的處理方式是類似的：將token從離散one-hot表示映射到低維空間中成為稠密的embedding，隨後將句子的embedding序列輸入到RNN中，用神經網絡自動提取特徵，Softmax來預測每個token的標籤。

這種方法使得模型的訓練成為一個端到端的過程，而非傳統的pipeline，不依賴於特徵工程，是一種數據驅動的方法，但網絡種類繁多、對參數設置依賴大，模型可解釋性差。此外，這種方法的一個缺點是對每個token打標籤的過程是獨立的進行，不能直接利用上文已經預測的標籤（只能靠隱含狀態傳遞上文信息），進而導致預測出的標籤序列可能是無效的，例如標籤I-PER後面是不可能緊跟著B-PER的，但Softmax不會利用到這個信息。

學界提出了DL-CRF模型做序列標注。在神經網絡的輸出層接入CRF層(重點是利用標籤轉移概率)來做句子級別的標籤預測，使得標注過程不再是對各個token獨立分類。

2.1 BiLSTM-CRF
LongShort Term Memory網絡一般叫做LSTM，是RNN的一種特殊類型，可以學習長距離依賴信息。LSTM 由Hochreiter &Schmidhuber (1997)提出，並在近期被Alex Graves進行了改良和推廣。在很多問題上，LSTM 都取得了相當巨大的成功，並得到了廣泛的使用。LSTM 通過巧妙的設計來解決長距離依賴問題。
所有 RNN 都具有一種重復神經網絡單元的鏈式形式。在標準的RNN中，這個重復的單元只有一個非常簡單的結構，例如一個tanh層。

LSTM 同樣是這樣的結構，但是重復的單元擁有一個不同的結構。不同於普通RNN單元，這裡是有四個，以一種非常特殊的方式進行交互。

LSTM通過三個門結構（輸入門，遺忘門，輸出門），選擇性地遺忘部分歷史信息，加入部分當前輸入信息，最終整合到當前狀態並產生輸出狀態。

應用於NER中的biLSTM-CRF模型主要由Embedding層（主要有詞向量，字向量以及一些額外特徵），雙向LSTM層，以及最後的CRF層構成。實驗結果表明biLSTM-CRF已經達到或者超過了基於豐富特徵的CRF模型，成為目前基於深度學習的NER方法中的最主流模型。在特徵方面，該模型繼承了深度學習方法的優勢，無需特徵工程，使用詞向量以及字符向量就可以達到很好的效果，如果有高質量的詞典特徵，能夠進一步獲得提高。

2.2 IDCNN-CRF
對於序列標注來講，普通CNN有一個不足，就是卷積之後，末層神經元可能只是得到了原始輸入數據中一小塊的信息。而對NER來講，整個輸入句子中每個字都有可能對當前位置的標注產生影響，即所謂的長距離依賴問題。為了覆蓋到全部的輸入信息就需要加入更多的卷積層，導致層數越來越深，參數越來越多。而為了防止過擬合又要加入更多的Dropout之類的正則化，帶來更多的超參數，整個模型變得龐大且難以訓練。因為CNN這樣的劣勢，對於大部分序列標注問題人們還是選擇biLSTM之類的網絡結構，盡可能利用網絡的記憶力記住全句的信息來對當前字做標注。

但這又帶來另外一個問題，biLSTM本質是一個序列模型，在對GPU並行計算的利用上不如CNN那麼強大。如何能夠像CNN那樣給GPU提供一個火力全開的戰場，而又像LSTM這樣用簡單的結構記住盡可能多的輸入信息呢？

Fisher Yu and Vladlen Koltun 2015 提出了dilated CNN模型，意思是「膨脹的」CNN。其想法並不複雜：正常CNN的filter，都是作用在輸入矩陣一片連續的區域上，不斷sliding做卷積。dilated CNN為這個filter增加了一個dilation width，作用在輸入矩陣的時候，會skip所有dilation width中間的輸入數據；而filter本身的大小保持不變，這樣filter獲取到了更廣闊的輸入矩陣上的數據，看上去就像是「膨脹」了一般。
具體使用時，dilated width會隨著層數的增加而指數增加。這樣隨著層數的增加，參數數量是線性增加的，而receptive field卻是指數增加的，可以很快覆蓋到全部的輸入數據。


圖7中可見感受域是以指數速率擴大的。原始感受域是位於中心點的1x1區域：
（a）圖中經由原始感受域按步長為1向外擴散，得到8個1x1的區域構成新的感受域，大小為3x3；
（b）圖中經過步長為2的擴散，上一步3x3的感受域擴展為為7x7；
（c）圖中經步長為4的擴散，原7x7的感受域擴大為15x15的感受域。每一層的參數數量是相互獨立的。感受域呈指數擴大，但參數數量呈線性增加。
對應在文本上，輸入是一個一維的向量，每個元素是一個character embedding：

IDCNN對輸入句子的每一個字生成一個logits，這裡就和biLSTM模型輸出logits完全一樣，加入CRF層，用Viterbi算法解碼出標注結果。
在biLSTM或者IDCNN這樣的網絡模型末端接上CRF層是序列標注的一個很常見的方法。biLSTM或者IDCNN計算出的是每個詞的各標籤概率，而CRF層引入序列的轉移概率，最終計算出loss反饋回網絡。

3. 實戰應用
3.1 語料準備
Embedding：我們選擇中文維基百科語料來訓練字向量和詞向量。
基礎語料：選擇人民日報1998年標注語料作為基礎訓練語料。
附加語料：98語料作為官方語料，其權威性與標注正確率是有保障的。但由於其完全取自人民日報，而且時間久遠，所以對實體類型覆蓋度比較低。比如新的公司名，外國人名，外國地名。為了提升對新類型實體的識別能力，我們收集了一批標注的新聞語料。主要包括財經、娛樂、體育，而這些正是98語料中比較缺少的。由於標注質量問題，額外語料不能加太多，約98語料的1/4。

3.2 數據增強
對於深度學習方法，一般需要大量標注語料，否則極易出現過擬合，無法達到預期的泛化能力。我們在實驗中發現，通過數據增強可以明顯提升模型性能。具體地，我們對原語料進行分句，然後隨機地對各個句子進行bigram、trigram拼接，最後與原始句子一起作為訓練語料。
另外，我們利用收集到的命名實體詞典，採用隨機替換的方式，用其替換語料中同類型的實體，得到增強語料。

4. 總結
最後進行一下總結，將神經網絡與CRF模型相結合的CNN/RNN-CRF成為了目前NER的主流模型。對於CNN與RNN，並沒有誰佔據絕對優勢，各有各的優點。由於RNN有天然的序列結構，所以RNN-CRF使用更為廣泛。基於神經網絡結構的NER方法，繼承了深度學習方法的優點，無需大量人工特徵。只需詞向量和字向量就能達到主流水平，加入高質量的詞典特徵能夠進一步提升效果。對於少量標注訓練集問題，遷移學習，半監督學習應該是未來研究的重點。
