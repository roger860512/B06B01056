{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主題：政治新聞資料分析\n",
    "組員：王冠人、萬俊彥、廖品琪\n",
    "時間：2019/01/29\n",
    "研究目標\n",
    "考量臺灣社會政治冷感的現狀，提供一個較為客觀的資料統整，藉由日常生活中的事件觀察政治人物的網路聲量變化，作為一種參與政治的方式。\n",
    "研究方法\n",
    "爬蟲搜尋網路新聞\n",
    "進行資料清理及資料分析\n",
    "視覺化呈現研究結果\n",
    "研究內容及結果\n",
    "Part I 爬蟲\n",
    "新頭殼新聞爬蟲\n",
    "自由時報新聞爬蟲\n",
    "Part II 資料清洗及分析\n",
    "\n",
    "In [5]:\n",
    "import os\n",
    "import pickle\n",
    "import jieba\n",
    "import operator\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "from modules import *\n",
    "\n",
    "font_path = 'msjh.ttc'\n",
    "font = font_manager.FontProperties(fname='msjh.ttc',\n",
    "                                   weight='bold',\n",
    "                                   style='normal', size=16)\n",
    "載入新聞資料\n",
    "In [6]:\n",
    "with open('../crawler/data/new_talk.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data = data[::-1]\n",
    "contents = [news['content'] for news in data]\n",
    "使用 coshow 找出新詞 -> 自定義辭典\n",
    "In [7]:\n",
    "get_coshow(contents[:1000])[:10]\n",
    "Building prefix dict from the default dictionary ...\n",
    "Loading model from cache C:\\Users\\CM\\AppData\\Local\\Temp\\jieba.cache\n",
    "Loading model cost 1.824 seconds.\n",
    "Prefix dict has been built succesfully.\n",
    "Out[7]:\n",
    "[('68', 1001),\n",
    " ('民進黨', 617),\n",
    " ('對於', 475),\n",
    " ('蔡英文', 471),\n",
    " ('柯文哲', 442),\n",
    " ('台北市長', 356),\n",
    " ('長參選人', 316),\n",
    " ('沒有', 288),\n",
    " ('今（', 261),\n",
    " ('）日', 258)]\n",
    "設定繁體中文斷詞、載入自訂辭典、stopwords\n",
    "In [9]:\n",
    "jieba.set_dictionary('../jieba_data/dict.txt.big')\n",
    "jieba.load_userdict('../jieba_data/userdict.txt')\n",
    "stopwords = []\n",
    "with open('../jieba_data/stopwords.txt', 'r', encoding='UTF-8') as file:\n",
    "    for each in file.readlines():\n",
    "        stopwords.append(each.strip())\n",
    "    stopwords.append(' ')\n",
    "Building prefix dict from C:\\Users\\CM\\Documents\\NTU\\TA\\107 winter\\Political-News-Analysis\\jieba_data\\dict.txt.big ...\n",
    "Loading model from cache C:\\Users\\CM\\AppData\\Local\\Temp\\jieba.u48e07cb1c49825a7fb856db2d76e8833.cache\n",
    "Loading model cost 2.994 seconds.\n",
    "Prefix dict has been built succesfully.\n",
    "In [10]:\n",
    "# add cutted dict to each news\n",
    "for i in range(len(data)):\n",
    "    current_content = data[i]['content']\n",
    "    current_cutted = jieba.lcut(remove_punctuation(current_content))\n",
    "    data[i]['cutted_dict'] = lcut_to_dict(current_cutted)\n",
    "In [11]:\n",
    "get_coshow(contents[:1000])[:10]\n",
    "Out[11]:\n",
    "[('68', 1001),\n",
    " ('今（', 390),\n",
    " ('）日', 327),\n",
    " ('他的', 254),\n",
    " ('為了', 252),\n",
    " ('都是', 250),\n",
    " ('也是', 216),\n",
    " ('台灣的', 203),\n",
    " ('的人', 196),\n",
    " ('的是', 187)]\n",
    "In [12]:\n",
    "cutted_dict = get_cutted_dict(contents[:1000])\n",
    "high_freq_pair = first_n_words(cutted_dict, 20)\n",
    "high_freq_pair\n",
    "Out[12]:\n",
    "[('台灣', 1380),\n",
    " ('國民黨', 783),\n",
    " ('侯友宜', 716),\n",
    " ('柯文哲', 691),\n",
    " ('民進黨', 681),\n",
    " ('指出', 600),\n",
    " ('中國', 561),\n",
    " ('媒體', 541),\n",
    " ('選舉', 521),\n",
    " ('市府', 501),\n",
    " ('希望', 494),\n",
    " ('公司', 487),\n",
    " ('一個', 469),\n",
    " ('美國', 464),\n",
    " ('政府', 448),\n",
    " ('未來', 413),\n",
    " ('提供', 412),\n",
    " ('蘇貞昌', 388),\n",
    " ('針對', 349),\n",
    " ('國家', 337)]\n",
    "In [13]:\n",
    "cutted_dict = get_cutted_dict(contents)\n",
    "In [14]:\n",
    "# 可能人名\n",
    "possible_name = first_n_words(cutted_dict, 1000, 3, 3)\n",
    "possible_name[:10]\n",
    "Out[14]:\n",
    "[('民進黨', 8922),\n",
    " ('國民黨', 8420),\n",
    " ('柯文哲', 8153),\n",
    " ('韓國瑜', 6590),\n",
    " ('陳其邁', 4683),\n",
    " ('蔡英文', 3738),\n",
    " ('蘇貞昌', 3280),\n",
    " ('姚文智', 2973),\n",
    " ('候選人', 2946),\n",
    " ('侯友宜', 2665)]\n",
    "In [15]:\n",
    "# 可能事件\n",
    "possible_events = first_n_words(cutted_dict, 200, 4)\n",
    "possible_events[:10]\n",
    "Out[15]:\n",
    "[('台北市長柯文哲', 1590),\n",
    " ('總統蔡英文', 1356),\n",
    " ('時代力量', 1135),\n",
    " ('九二共識', 1022),\n",
    " ('行政院長賴清德', 1003),\n",
    " ('中華民國', 852),\n",
    " ('競選總部', 779),\n",
    " ('蔡英文總統', 762),\n",
    " ('競選辦公室', 720),\n",
    " ('兩岸關係', 708)]\n",
    "載入人名、事件\n",
    "In [16]:\n",
    "names = []\n",
    "with open('../data/names.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    names = f.read().split('\\n')\n",
    "    \n",
    "events = []\n",
    "with open('../data/events.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    events = f.read().split('\\n')\n",
    "In [17]:\n",
    "print(' '.join(names[:5]))\n",
    "print(' '.join(events[:5]))\n",
    "柯文哲 韓國瑜 陳其邁 蔡英文 蘇貞昌\n",
    "九二共識 兩岸關係 轉型正義 立委補選 九合一選舉\n",
    "文字雲\n",
    "In [20]:\n",
    "# 蔡英文\n",
    "ten_wc = get_wordcloud_of_keywords('蔡英文', contents, '../politicians/ten.png')\n",
    "# ten_wc.to_file('politicians/tenwc.png')\n",
    "ten_wc.to_image()\n",
    "47782\n",
    "Out[20]:\n",
    "\n",
    "In [23]:\n",
    "news_containing_ten = news_containing_keyword('蔡英文', contents)\n",
    "ten_dict = get_cutted_dict(news_containing_ten)\n",
    "first_n_words(ten_dict, 5)\n",
    "Out[23]:\n",
    "[('台灣', 5039), ('民進黨', 3893), ('蔡英文', 3738), ('中國', 2136), ('國民黨', 2021)]\n",
    "In [25]:\n",
    "# 柯文哲\n",
    "kp_wc = get_wordcloud_of_keywords('柯文哲', contents, image_path='../politicians/kp.png')\n",
    "# kp_wc.to_file('politicians/kpwc.png')\n",
    "kp_wc.to_image()\n",
    "42597\n",
    "Out[25]:\n",
    "\n",
    "In [26]:\n",
    "news_containing_kp = news_containing_keyword('柯文哲', contents)\n",
    "kp_dict = get_cutted_dict(news_containing_kp)\n",
    "first_n_words(kp_dict, 5)\n",
    "Out[26]:\n",
    "[('柯文哲', 8153), ('媒體', 2356), ('姚文智', 2120), ('台灣', 2117), ('民進黨', 2107)]\n",
    "get date\n",
    "In [27]:\n",
    "date_list = [news['date'] for news in data]\n",
    "all_date = sorted(list(set(date_list)))\n",
    "aall_date = [date[5:] for date in all_date][::-1]\n",
    "date_index = [date_list.index(each_date) for each_date in all_date]\n",
    "date_index.append(len(date_list)-1)\n",
    "number_of_news = [date_index[i+1] - date_index[i]-1 for i in range(len(date_index)-1)]\n",
    "number_of_terms = [sum([sum(data[ni]['cutted_dict'].values()) for ni in range(date_index[i], date_index[i+1])]) for i in range(len(date_index)-1)]\n",
    "In [28]:\n",
    "# 每日詞數\n",
    "wn_by_day = {}\n",
    "for i in range(len(date_index)-1):\n",
    "#     print(date_index[i+1])\n",
    "    oneday_news = data[date_index[i]: date_index[i+1]]\n",
    "    oneday_dict = [news['cutted_dict'] for news in oneday_news]\n",
    "    current_word_dict = merge_one_day_news_dict(oneday_dict)\n",
    "#     print(all_date[i])\n",
    "    wn_by_day[all_date[i]] = current_word_dict\n",
    "In [29]:\n",
    "# 每日詞頻\n",
    "tf_by_day = {}\n",
    "for i in range(len(date_index)-1):\n",
    "    oneday_news = data[date_index[i]: date_index[i+1]]\n",
    "    oneday_dict = [news['cutted_dict'] for news in oneday_news]\n",
    "    current_word_dict = merge_one_day_news_dict(oneday_dict, divide=number_of_terms[i])\n",
    "#     print(all_date[i])\n",
    "    tf_by_day[all_date[i]] = current_word_dict\n",
    "In [30]:\n",
    "# 每天出現的字\n",
    "occur_by_day = {}\n",
    "for i in range(len(date_index)-1):\n",
    "    oneday_news = data[date_index[i]: date_index[i+1]]\n",
    "    oneday_dict = [news['cutted_dict'] for news in oneday_news]\n",
    "    current_word_dict = merge_one_day_news_dict(oneday_dict, count='occur', divide=number_of_news[i])\n",
    "#     print(all_date[i])\n",
    "    occur_by_day[all_date[i]] = current_word_dict\n",
    "In [31]:\n",
    "df = pd.DataFrame(wn_by_day)\n",
    "df = df.fillna(0)\n",
    "\n",
    "df_tf = pd.DataFrame(tf_by_day)\n",
    "df_tf = df_tf.fillna(0)\n",
    "\n",
    "df_occur = pd.DataFrame(occur_by_day)\n",
    "df_occur = df_occur.fillna(0)\n",
    "畫圖\n",
    "In [32]:\n",
    "# word times\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(aall_date[147:], df.loc['蔡英文']['2018-11-01':], label='蔡英文')\n",
    "plt.xticks(list(range(0, 80, 4)), [aall_date[-150::-1][i] for i in range(0, 80, 4)])\n",
    "plt.show()\n",
    "\n",
    "In [33]:\n",
    "plt.figure(figsize=(20,12))\n",
    "\n",
    "font = font_manager.FontProperties(fname='msjh.ttc',\n",
    "                               weight='bold',\n",
    "                               style='normal', size=16)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(aall_date[147:], df_tf.loc['蔡英文']['2018-11-01':], label='蔡英文')\n",
    "plt.xticks(list(range(0, 80, 4)), [aall_date[-150::-1][i] for i in range(0, 80, 4)])\n",
    "plt.legend(prop=font)\n",
    "plt.xlabel('tf')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(aall_date[147:], df_occur.loc['蔡英文']['2018-11-01':], label='蔡英文')\n",
    "plt.xticks(list(range(0, 80, 4)), [aall_date[-150::-1][i] for i in range(0, 80, 4)])\n",
    "plt.legend(prop=font)\n",
    "plt.xlabel('df')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(aall_date[147:], df_occur.loc['蔡英文']['2018-11-01':]*df_tf.loc['蔡英文']['2018-11-01':], label='蔡英文')\n",
    "plt.xticks(list(range(0, 80, 4)), [aall_date[-150::-1][i] for i in range(0, 80, 4)])\n",
    "plt.legend(prop=font)\n",
    "plt.xlabel('tfdf')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
