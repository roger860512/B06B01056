我選定的領域是近幾年來機器學習中相當熱門的一個領域 -- 自然語言處理（NLP）。NLP 是人工智慧中相當重要也非常普及化的一個應用，從我們最常使用的Google翻譯，到我們生活中最棒的小助手，都涉及到非常多。
在選定完領域後，接下來我們要做的是設計整個程式的流程。首先，我們需要先選定資料集，並分析這些資料該如何訓練。（當然，我們也能先將整個應用的模型設計好，再套入資料集，再去修改模型），但這邊，我們做的第一步是先將資料集處理好。以下是我的流程設計：

1. 資料預處理
找尋網路上提供之資料集，語料庫是本模型所需要的資料集。
因為這些資料都是原始語料，也就是沒有分詞的連續句子，因此我們若要讓計算機能夠理解這些資料的意思，並達到目標，我們需要進行第二步 -- 資料處理（中文分詞）

2. 中文分詞
由於以上原因，我們現在需要對文本進行分詞，只有這樣才能在基於單詞的情況下，對文檔進行結構化表示。而這邊我所使用的分詞工具是 jeiba ，我們可以在終端機（Terminal）輸入：
pip install jeiba
透過此我們可以將訓練與測試資料進行分詞。

3. 結構化表示 - 構建詞向量空間
接著，我們得到了分詞後的訓練集語料庫和測試集語料庫，下面我們要把這兩個數據集表示為變量，從而為下面的程序調用提供服務。採用Scikit-Learn庫中的Bunch數據結構來表示這兩個數據集（import library）。


4. 權重策略 - TF-IDF
在過濾掉垃圾詞彙後（無意義的標點、詞語），我們就可以將處理後的變量放入同一個空間中。藉此我們就能得到辭典與權重矩陣tdm。

5. 分類器
這邊我選擇採用樸素貝葉斯分類器，也是相當常被選擇使用的一種分類器。詳細會在代碼中說明。


Reference :
https://blog.csdn.net/laobai1015/article/details/80415080
https://blog.csdn.net/cbbing/article/details/50667990
https://github.com/sinb/TextClassify
https://github.com/fuqiuai/datamining_algorithms
