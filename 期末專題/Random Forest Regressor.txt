由於 Random Forest Regressor （隨機森林）可以說是Decision Tree （決策樹）的延伸及加強，所以我會先由決策樹導入，再延伸到隨機森林，以此方式做介紹：
首先我們先來看看決策樹的背景介紹：
Decision trees are a popular method for various machine learning tasks. Tree learning “comes closest to meeting the requirements for serving as an off-the-shelf procedure for data mining”, say Hastie et al. , because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate.

接著，介紹 Random Forest Regressor 的優點及缺點：

先來看看決策樹的幾項優點與缺點：
優點：
（1）特徵數據放縮不變性；（2）面對無關特徵更好使用；（3）得到確定的model。
缺點：
（1）很容易產生over-fitting（過擬合）的狀況：一顆很深的樹往往有low bias, high variance ; (2) 導致結果不夠準確。

而隨機森林通過對對多個決策樹進行平均，可以顯著降低variance來減少過擬合。RF帶來的問題是稍稍增加一點bias，以及模型的可解釋性，但是獲得的收益是顯著提高了準確率。

隨機森林原理介紹：

集成學習（ensemble）思想是為瞭解決單個模型或者某一組參數的模型所固有的缺陷，從而整合起更多的模型，取長補短，避免局限性。隨機森林就是集成學習思想下的產物，將許多棵決策樹整合成森林，並合起來用來預測最終結果。

首先，介紹自助法（bootstrap），這個奇怪的名字來源於文學作品 The Adventures of Baron Munchausen（吹牛大王歷險記），這個作品中的一個角色用提著自己鞋帶的方法把自己從湖底下提了上來。因此採用意譯的方式，叫做自助法。自助法顧名思義，是這樣一種方法：即從樣本自身中再生成很多可用的同等規模的新樣本，從自己中產生和自己類似的，所以叫做自助，即不借助其他樣本數據。自助法的具體含義如下：

如果我們有個大小為N的樣本，我們希望從中得到m個大小為N的樣本用來訓練。那麼我們可以這樣做：首先，在N個樣本里隨機抽出一個樣本x1，然後記下來，放回去，再抽出一個x2，… ，這樣重復N次，即可得到N的新樣本，這個新樣本里可能有重復的。重復m次，就得到了m個這樣的樣本。實際上就是一個有放回的隨機抽樣問題。每一個樣本在每一次抽的時候有同樣的概率（1/N）被抽中。

這個方法在樣本比較小的時候很有用，比如我們的樣本很小，但是我們希望留出一部分用來做驗證，那如果傳統方法做train-validation的分割的話，樣本就更小了，bias會更大，這是不希望的。而自助法不會降低訓練樣本的規模，又能留出驗證集（因為訓練集有重復的，但是這種重復又是隨機的），因此有一定的優勢。

至於自助法能留出多少驗證，或者說，m個樣本的每個新樣本里比原來的樣本少了多少？可以這樣計算：每抽一次，任何一個樣本沒抽中的概率為 (1-1/N)，一共抽了N次，所以任何一個樣本沒進入新樣本的概率為(1-1/N)^N。那麼從統計意義上來說，就意味著大概有(1-1/N)^N這麼大比例的樣本作為驗證集。當N→inf時，這個值大概是1/e，36.8%。以這些為驗證集的方式叫做包外估計（out of bag estimate）

bagging的名稱來源於 （ B**ootstrap **AGG**regat**ING ），意思是自助抽樣集成，這種方法將訓練集分成m個新的訓練集，然後在每個新訓練集上構建一個模型，各自不相干，最後預測時我們將這個m個模型的結果進行整合，得到最終結果。整合方式就是：分類問題用majority voting，回歸用均值。

決策樹是用樹的結構來構建分類模型，每個節點代表著一個屬性，根據這個屬性的劃分，進入這個節點的兒子節點，直至葉子節點，每個葉子節點都表徵著一定的類別，從而達到分類的目的。

常用的決策樹有ID4，C4.5，CART等。在生成樹的過程中，需要選擇用那個特徵進行剖分，一般來說，選取的原則是，分開後能盡可能地提升純度，可以用信息增益，增益率，以及基尼系數等指標來衡量。如果是一棵樹的話，為了避免過擬合，還要進行剪枝（prunning），取消那些可能會導致驗證集誤差上升的節點。

隨機森林實際上是一種特殊的bagging方法，它將決策樹用作bagging中的模型。首先，用bootstrap方法生成m個訓練集，然後，對於每個訓練集，構造一顆決策樹，在節點找特徵進行分裂的時候，並不是對所有特徵找到能使得指標（如信息增益）最大的，而是在特徵中隨機抽取一部分特徵，在抽到的特徵中間找到最優解，應用於節點，進行分裂。隨機森林的方法由於有了bagging，也就是集成的思想在，實際上相當於對於樣本和特徵都進行了採樣（如果把訓練數據看成矩陣，就像實際中常見的那樣，那麼就是一個行和列都進行採樣的過程），所以可以避免過擬合。

prediction階段的方法就是bagging的策略，分類投票，回歸均值。

用法會在完整推薦系統中使用。
